\section{Analysis}
\label{sec:analysis}

One goal of this study is to determine whether current CMS data are sufficient to impose visible constraints on the pMSSM parameter space, and consequently on SUSY mass scales and low energy observables such as $(g-2)_\mu$, BR($b\rightarrow s\gamma$), etc.  To achieve this goal, we carry out the following analysis steps:
\begin{itemize}
\item For each of the 6K pMSSM points we generate 10K events.
\item We perform three blessed CMS analyses, namely the
 ``di-jet $\alpha_T$" (Had), ``opposite-sign di-lepton" (OS) and ``same-sign di-lepton" (SS) 
 analyses on each of the 6K pMSSM samples.
\item For each pMSSM point and for each of the three analyses, we combine the 
predicted signal yield $s$ with the approved CMS results---the
observed event count $N$ and 
a data-driven background estimate $b \pm \delta b$---to compute the likelihood as a function
of the pMSSM parameters. 
\item We weight every pMSSM point with the product of the three likelihoods, one  for each
of the three analyses. Since the pMSSM points were sampled from a (bounded) 
uniform  19-dimensional density, the set of weights constitute a non-parametric 
representation of the likelihood function. 
\item In order to illustrate the contribution of the current CMS data, we calculate profile likelihoods 
for the relevant model parameters, with and without the CMS likelihood, that is, the weights.  \end{itemize}

\subsection{Event samples}

We use a 6K subset of the pMSSM points generated by Berger \emph{et al.} 
as explained in Section~\ref{sec:model}.  Information on each point is contained in a SUSY Les Houches (SLHA)~\cite{Skands:2003cj} file.  For each point, 10K events were generated using {\tt PYTHIA6}~\cite{Sjostrand:2006za}.  We simulate the response of the CMS detector using
the publicly available general purpose detector simulation package {\tt Delphes}~\cite{Ovyn:2009tx}.  Through extensive numerical and shape comparisons we
conclude that we are able to provide a \emph{very} fast simulation of the CMS detector 
that is good to within 10\%. An accurate,  fast, simulation is critical for studies
of this scope. For the leptonic studies the agreement is within 25\%. Further details on {\tt CMSSW} fullsim - {\tt Delphes} comparison can be found in Appendix~\ref{sec:cmsdelphescomp}.
During the next iteration of this study, we plan to 
use the efficiency model outlined in ~\cite{sspaper}.

\subsection{The three CMS analyses}

We used the following three blessed CMS SUSY analyses designed for the early 35 pb$^{-1}$ CMS 7 TeV data  collected in 2010:
\begin{itemize}
\item Di-jet $\alpha_T$ analysis, investigating the multijets plus missing energy final state with emphasis on the $\alpha_T$ variable proposed by Randall andTucker-Smith
\item Opposite-sign di-lepton analysis, investigating the opposite-sign di-lepton plus missing energy final state
\item Same-sign di-lepton analysis, investigating the same-sign di-lepton plus missing energy final state.
\end{itemize}

Using these analyses, we obtained the observed event counts, background estimates and signal counts, as described in Sections~\ref{sec:dbgcount} and~\ref{sec:sigyield}, all of which will be used as inputs for the 

\subsubsection{Observed event counts and background estimates}
\label{sec:dbgcount}

The $\alpha_T$ and two di-lepton analyses were developed in detail by CMS SUSY RA1 and RA6 groups respectively and were implemented on the 7 TeV 35 pb$^{-1}$ CMS data and 7 TeV SM and SUSY Monte Carlo generated and simulated by {\it CMSSW} fullsim centrally in 2010.

We will simply take the observed event counts and background estimates directly from the official results of these analyses.  The numbers are summarized in Table~\ref{tab:dbgcount}.

\begin{table}[htdp]
\caption{Results of the three CMS analyses for 35 pb$^{-1}$.}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Analysis & Observed    & Data-driven SM & MC SM BG \\
              & event count & BG estimate      & prediction    \\
(k)          & ($N_k$)     & ($b_k \pm \delta b_k$) & ($b_k^{MC} \pm b_k^{MC}$) \\
\hline              
Dijet $\alpha_T$ & 14 & 11.4 $\pm$ 2.0 & 9.2 $\pm$ 0.9 \\
OS di-lepton & 1 & 2.1 $\pm$ 2.1 & 1.27 \\
SS di-lepton & 0 & 1.2 $\pm$ 0.8 & 0.35 \\
\hline
\end{tabular}
\end{center}
\label{tab:dbgcount}
\end{table}%


\subsubsection{Signal yields}
\label{sec:sigyield}

For each of the 6K pMSSM points, we performed the three analyses on the associated 10K simulated and reconstructed events. This results in three expected signal yields for every pMSSM point. 


\subsection{Calculation of the likelihood function}

Every analysis $k$ yields an observed count $N_k$ and a background estimate
$b_k \pm \delta b_k$ where $k=1,2,3$.  We make the standard assumption that the
counts are Poisson distributed in which case the total likelihood from the combination of the three analyses can be written as
\begin{equation}
L(\theta) \equiv p(N|s,b) = \prod_{k=1}^3 \mbox{Poisson}(N_k | s_k + b_k),
\end{equation}
where $s_k$ is the predicted signal and $L(\theta)$ the likelihood
written as a function of the  $d = $19-dimensional pMSSM parameter $\theta$. 
In this study, the
 map of $\theta$ to the expected signals $s_k$ is represented non-parametrically
 using the 6K pMSSM points. In principle, we could build a smooth functional approximation to
 this map; however, we have not attempted this in the current study. 
 We incorporate the joint likelihood function of the three CMS analyses by
 weighting the $i$th pMSSM point by the likelihood 
 value $L(\theta_i)$ for that point. Since the sampling of points is uniform, the swarm of weighted
 points constitute a non-parametric representation of $L(\theta)$.
 
 For any likelihood function, exact confidence regions~\cite{James}  can \emph{always} 
 be created in
 the unrestricted parameter space. However, such
 confidence regions are seldom useful when the dimensionality of the 
 parameter space is large. It is 
 more useful to examine parameters one or two at a time. In the current study,
 we examine each parameter separately using the profile likelihood, a broadly applicable frequentist
 construct.

\subsection{Calculation of profile likelihoods}
As noted above, an important goal of this study is to determine to which pMSSM parameters the current  CMS data-set is most sensitive and to what degree. This is useful because we shall
be able to make statements whose validity is much broader than those made using
models in which strong, but weakly motivated, simplifying assumptions have been made.  A well-established way of conveying relevant information about the parameters of interest
 is to display their 1-dimensional profile likelihoods. We do this for each pMSSM parameter
as well as for some observables.

Suppose we wish to investigate the gluino mass parameter $M_3$. Its 
(1-dimensional) profile likelihood, $L_p$,~\cite{James} is defined by
\begin{equation}
 L_p(M_3) \equiv L(M_3, \hat{\theta}_2(M_3),\cdots, \hat{\theta}_{19}(M_3)),
\end{equation}
where $\hat{\theta}_2\cdots\hat{\theta}_{19}$ denote the maximum likelihood 
estimates (MLE), for a given value of $M_3$, of the remaining 18 parameters. 
Since we do not have a functional approximation of the map from $\theta$ to the
expected signals, our likelihood
function $L(\theta)$ is available non-parametrically as a swarm of weighted points, so standard
maximizing programs such as {\tt MINUIT} cannot be used. We have therefore devised
our own non-parametric profiling algorithm, which proceeds as follows:
\begin{enumerate}
\item For each 1-dimensional bin of the parameter or observable of interest,
for example $M_3$, we create a $d$-dimensional histogram of the 6K points using the Root class {\tt TKDTreeBinning}. Through recursive binary partitioning, this class creates $d$-dimensional bins with equal bin content.
\item For a given bin, e.g. in the gluino mass parameter $M_3$, we find which of the 6K points is consistent with the given value of $M_3$.
\item Using these points, we find the bin with the maximum density (the one with the smallest
volume) and use that as an estimate of the profile likelihood value for the given value of $M_3$. 
\item In addition, in order to provide a better estimate, the above procedure is repeated 100 times, each with a different bootstrap sample of the original 6K points and the average is taken over the profiles in each bin. This in turn is done for each of the $\sim 70$ variables under investigation.
\end{enumerate}


