\section{Analysis}
\label{sec:analysis}

One goal of this study is to determine whether current CMS data are sufficient to impose visible constraints on the pMSSM parameter space, and consequently on SUSY mass scales and low energy observables such as $(g-2)_\mu$, BR($b\rightarrow s\gamma$), etc.  To achieve this goal, we carry out the following analysis steps:
\begin{itemize}
\item For each of the 6K pMSSM points we generate 10K events.
\item We perform three approved CMS analyses, namely the
 ``di-jet $\alpha_T$" (Had), ``opposite-sign di-lepton" (OS) and ``same-sign di-lepton" (SS) 
 analyses on each of the 6K pMSSM samples.
\item For each pMSSM point and for each of the three analyses, we combine the 
predicted signal yield $s$ with the approved CMS results---the
observed event count $N$ and 
a data-driven background estimate $b \pm \delta b$---to compute the likelihood as a function
of the pMSSM parameters. 
\item We weight every pMSSM point with the product of the three likelihoods, one  for each
of the three analyses. Since the pMSSM points were sampled from a (bounded) 
uniform  19-dimensional density, the set of weights constitute a non-parametric 
representation of the likelihood function. 
\item In order to illustrate the contribution of the current CMS data, we calculate profile likelihoods 
for the relevant model parameters, with and without the CMS likelihood, that is, the weights.  \end{itemize}

\subsection{Event samples}

We use a 6K subset of the pMSSM points generated by Berger \emph{et al.} 
as explained in Section~\ref{sec:model}.  Information on each point is contained in a SUSY Les Houches (SLHA)~\cite{Skands:2003cj} file.  For each point, 10K events were generated using {\tt PYTHIA6}~\cite{Sjostrand:2006za}.  We simulate the response of the CMS detector using
the publicly available general purpose detector simulation package {\tt Delphes}~\cite{Ovyn:2009tx}.  Through extensive numerical and shape comparisons we
conclude that we are able to provide a \emph{very} fast simulation of the CMS detector 
that is good to within 10\%. An accurate,  fast, simulation is critical for studies
of this scope. For the leptonic studies the agreement is within 25\%. 
During the next iteration our this study, we plan to 
use the efficiency model outlined in ~\cite{sspaper}.

\subsection{Implementation of three blessed CMS analyses}

For each of the 6K pMSSM points, the hadronic and multi-lepton analyses are performed on the simulated and reconstructed 
events in order to compute the expected signal yields. The hadronic analysis agrees with the full simulation results within 10\%. Details on 
comparison between the reconstructed quantities can be found in the Section~\ref{sec:compare}.


The following table shows the Numerical results of the three CMS analyses for 35 pb$^{-1}$ of luminosity.

\begin{table}[htdp]
\caption{Numerical results of the three CMS analyses for 35 pb$^{-1}$ as given by {\tt CMSSW} full simulation and {\tt Delphes} for the SUSY benchmarks LM0 and LM1.}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Analysis & Observed  & Data-driven SM & MC SM BG \\
              & data count & BG estimate      & prediction    \\
(k)          & ($N_k$)     & ($b_k \pm \delta b_k$) & ($b_k^{MC} \pm b_k^{MC}$) \\
\hline              
Dijet $\alpha_T$ & 14 & 11.4 $\pm$ 2.0 & 9.2 $\pm$ 0.9 \\
OS di-lepton & 1 & 2.1 $\pm$ 2.1 & 1.27 \\
SS di-lepton & 0 & 1.2 $\pm$ 0.8 & 0.35 \\
\hline
\end{tabular}
\end{center}
\label{default}
\end{table}%


\subsection{Calculation of the likelihood function}

Every analysis $k$ yields an observed count $N_k$ and a background estimate
$b_k \pm \delta b_k$ where $k=1,2,3$.  We make the standard assumption that the
counts are Poisson distributed in which case the total likelihood from the combination of the three analyses can be written as
\begin{equation}
L(\theta) \equiv p(N|s,b) = \prod_{k=1}^3 \mbox{Poisson}(N_k | s_k + b_k),
\end{equation}
where $s_k$ is the predicted signal and $L(\theta)$ the likelihood
written as a function of the  $d = $19-dimensional pMSSM parameter $\theta$. 
In this study, the
 map of $\theta$ to the expected signals $s_k$ is represented non-parametrically
 using the 6K pMSSM points. In principle, we could build a smooth functional approximation to
 this map; however, we have not attempted this in the current study. The likelihood 
 values $L(\theta_i)$,
 where $i$ is the ith pMSSM point, is used as the weight for this point. This is how we 
 incorporate the likelihood function of the three approved CMS analyses in our
 pMSSM interpretation
 of the CMS results. 
 
 For any likelihood function, exact confidence regions~\cite{James}  can \emph{always} 
 be created in
 the unrestricted parameter space. However, such
 confidence regions are seldom useful when the dimensionality of the 
 parameter space is large. It is 
 more useful to examine parameters one or two at a time. In the current study,
 we examine each parameter separately using the profile likelihood, a broadly applicable frequentist
 construct.

\subsection{Calculation of profile likelihoods}
As noted above, an important goal of this study is to determine to which pMSSM parameters the current  CMS data-set is most sensitive and to what degree. This is useful because we shall
be able to make statements whose validity is much broader than those made using
models in which strong, but weakly motivated, simplifying assumptions have been made.  A well-established way of conveying relevant information about the parameters of interest
 is to display their 1-dimensional profile likelihoods. We do this for each pMSSM parameter
as well as for some observables.

Suppose we wish to investigate the gluino mass parameter $M_3$. Its 
(1-dimensional) profile likelihood, $L_p$,~\cite{James} is defined by
\begin{equation}
 L_p(M_3) \equiv L(M_3, \hat{\theta}_2(M_3),\cdots, \hat{\theta}_{19}(M_3)),
\end{equation}
where $\hat{\theta}_2\cdots\hat{\theta}_{19}$ denote the maximum likelihood 
estimates (MLE), for a given value of $M_3$, of the remaining 18 parameters. 
Since we do not have a functional approximation of the map from $\theta$ to the
expected signals, our likelihood
function $L(\theta)$ is available non-parametrically as a swarm of weighted points, so standard
maximizing programs such as {\tt MINUIT} cannot be used. We have therefore devised
our own non-parametric profiling algorithm, which proceeds as follows:
\begin{enumerate}
\item For each 1-dimensional bin of the parameter or observable of interest,
for example $M_3$, we create a $d$-dimensional histogram of the 6K points using the Root class {\tt TKDTreeBinning}. Through recursive binary partitioning, this class creates $d$-dimensional bins with equal bin content.
\item For a given bin, e.g. in the gluino mass parameter $M_3$, we find which of the 6K points is consistent with the given value of $M_3$.
\item Using these points, we find the bin with the maximum density (the one with the smallest
volume) and use that as an estimate of the profile likelihood value for the given value of $M_3$. 
\item In addition, in order to provide a better estimate, the above procedure is repeated 100 times, each with a different bootstrap sample of the original 6K points and the average is taken over the profiles in each bin. This in turn is done for each of the $\sim 70$ variables under investigation.
\end{enumerate}


