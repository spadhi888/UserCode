\documentclass[a4paper]{jpconf}
\usepackage{graphicx}
\begin{document}
\title{Use of glide-ins in CMS for production and analysis \jpcs}

\author{Daniel C Bradley, Oliver Gutsche, Kristian Hahn, Burt Holzman, Sanjay Padhi, Haifeng Pi, Igor Sfiligoi, Eric Vandering, Frank W\"urthwein (Any one else ??)}

\address{UWM, FNAL, MIT, UCSD}

\ead{cms-dct-wms@fnal.gov}

\begin{abstract}
With the evolution of various grid federations, the Condor glide-ins represent a key feature in providing a homogeneous pool of resources using late-binding technology. The CMS collaboration uses the glide-in based Workload Management System, glideinWMS, for production (ProdAgent) and distributed analysis (CRAB) of the data. The Condor glide-in daemons traverse to the worker nodes, submitted via Condor-G. Once activated, they preserve the Master-Worker relationships, with the worker first validating the execution environment on the worker node before pulling the jobs sequentially until the expiry of their lifetimes. The combination of late-binding and validation significantly reduces the overall failure rate visible to CMS physicists. We discuss the extensive use of the glideinWMS since the computing challenge, CCRC08, in order to prepare for the forthcoming LHC data-taking period. The key features essential to the success of large-scale production and analysis at CMS resources across major grid federations, including EGEE, OSG and NorduGrid are outlined. Use of glide-ins via the CRAB server mechanism and ProdAgent as well as first hand experience of using the next generation CREAM computing element within the CMS framework is also discussed.
\end{abstract}

\section{Introduction}

The CMS collaboration has adopted Grid computing as its base computing model to simplify the deployment and management of the
tens of thousand CPUs needed to accomplish its mission.
While the Grid computing paradigm has proven to be a boon for resource providers, 
allowing them to keep their administrative autonomy over the resources they manage,
the added abstraction layer has introduced several problems for the users, 
ranging from higher complexity to decreased reliability.

One solution that has proven to significantly reduce user problems is the late-binding, or pilot technology.  
A late-binding Workload Management System (WMS) hides the complexity of the Grid environment by dynamically creating 
a virtual private pool of compute resources, thus giving users an environment similar to a dedicated batch cluster.
This paper describes the experience of the CMS collaboration with one late-binding WMS implementation called glideinWMS. 


\section{Late binding based Workload Management System - GlideinWMS }

The Grid paradigm calls for the compute resources to be partitioned into multiple independent pools, called Grid sites,
with only a thin common layer to provide interoperability. 
Without some additional tools, this approach makes the life of a Grid user quite unpleasant. 

The four major problems the users experience are:
\begin {itemize}
\item 
A user must partition his/her jobs between the resource pools.
Finding the optimal partition is far from an easy task, as explained below.
\item
At any Grid site, the common layer provides only very limited information about the status and policies of the
batch system that handles the local resource pool.
This is a necessary evil that allows the common layer to present the information from all the different batch system 
implementations in a uniform way. 
\item 
The common layer provides only very limited information about the progress of a job, once it is accepted at a Grid site.
Again, this is a necessary evil that allows the common layer to monitor jobs submitted to different batch systems.  
\item
Each Grid site is allowed to configure the worker nodes the way it likes, within very permissive limits.
Users are expected to write their compute jobs in such a way to automatically adapt to any condition they encounter.
\end{itemize}

The approach taken by the late binding Workload Management Systems (WMS) to ease the user burden is to insulate the 
users from the Grid layer and create a dynamic virtual private pool of compute resources.
By doing this, the user gets the impression of running on a single, local, dedicated pool of compute resources
thus elimitating, or at least reducing the magnitude of the above mentioned major problems:
\begin {itemize}
\item 
By having a single resource pool, no job partitioning is needed.
\item
Detailed information about the status and policies of the virtual private pool can be made available, 
since the user can use the tool provided by the specific implementation of the used late binding WMS instance.
\item 
Detailed information about the progress of a job can be made available,
since the user can use the tool provided by the specific implementation of the used late binding WMS instance.
\item
The late binding WMS can reduce the hereogeneity of the compute resources, by either only gathering well defined ones,
or by providing wrapper scripts that provide common tools and libraries.
\end{itemize}


\subsection {Interoperability between EGEE, OSG, and NorduGrid}



\subsection {Scalability of the system}



\section{Use of glideinWMS for Monte Carlo Production and Data Reprocessing }



\section{Use of glideinWMS for data analysis}



\subsection{CMS User Analysis and CCRC-08}



\subsection{User analysis using Crabserver}



\subsection{User level MC production using Crabserver}



\subsection{Recent results and JobRobots}



\section{Coherent monitoring interface for the system}



\section{Experience using next generation CREAM CE}



\end{document}


